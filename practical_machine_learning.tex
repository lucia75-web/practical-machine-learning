\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={Practical Machine Learning Project},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\title{Practical Machine Learning Project}
\author{}
\date{\vspace{-2.5em}}

\begin{document}
\maketitle

\subsubsection{Overview}\label{overview}

The goal of your project is to predict the manner in which they did the
exercise. This is the ``classe'' variable in the training set. You may
use any of the other variables to predict with. A report describing has
been created to show how to built the model, how to use cross
validation.

\subsubsection{Load the Library}\label{load-the-library}

Check for missing dependencies and load necessary R packages

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{setwd}\NormalTok{(}\StringTok{"~/Coursera/practical-machine-learning"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(knitr)}
\KeywordTok{library}\NormalTok{(caret)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: lattice
\end{verbatim}

\begin{verbatim}
## Loading required package: ggplot2
\end{verbatim}

\subsubsection{Read the test and training
dataset}\label{read-the-test-and-training-dataset}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{training <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"./pml-training.csv"}\NormalTok{)}
\NormalTok{testing <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"./pml-testing.csv"}\NormalTok{)}

\KeywordTok{dim}\NormalTok{(training)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 19622   160
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(testing)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  20 160
\end{verbatim}

\subsubsection{Prepare the data}\label{prepare-the-data}

The data are prepared for the analysis: the not applicable data and not
a number data are removed from the data read form the *.csv file

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# remove variables that are almost always NA}

\NormalTok{training <-}\StringTok{ }\NormalTok{training[, }\KeywordTok{colSums}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(training)) }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{] }

\NormalTok{classe <-}\StringTok{ }\NormalTok{training}\OperatorTok{$}\NormalTok{classe}
\NormalTok{trainRemove <-}\StringTok{ }\KeywordTok{grepl}\NormalTok{(}\StringTok{"^X|timestamp|window"}\NormalTok{, }\KeywordTok{names}\NormalTok{(training))}
\NormalTok{training <-}\StringTok{ }\NormalTok{training[, }\OperatorTok{!}\NormalTok{trainRemove]}
\NormalTok{training <-}\StringTok{ }\NormalTok{training[, }\KeywordTok{sapply}\NormalTok{(training, is.numeric)]}
\NormalTok{training}\OperatorTok{$}\NormalTok{classe <-}\StringTok{ }\NormalTok{classe}
\end{Highlighting}
\end{Shaded}

\subsubsection{Split the data in two
subset}\label{split-the-data-in-two-subset}

Split the training data in two dataset, a 70\% is used for training and
a 30\% is used to verified the predicted model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datasplitted <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(training}\OperatorTok{$}\NormalTok{classe, }\DataTypeTok{p=}\FloatTok{0.70}\NormalTok{, }\DataTypeTok{list=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{mytrainData <-}\StringTok{ }\NormalTok{training[datasplitted, ]}
\NormalTok{mytestData <-}\StringTok{ }\NormalTok{training[}\OperatorTok{-}\NormalTok{datasplitted, ]}
\end{Highlighting}
\end{Shaded}

\subsubsection{Data Modeling}\label{data-modeling}

Fit the predictive model. Two model are estimated to find the best
model, the randoms forest and the conditional Inference Tree

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modelTree <-}\StringTok{ }\KeywordTok{train}\NormalTok{(classe }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data=}\NormalTok{mytrainData, }\DataTypeTok{method=}\StringTok{"ctree"}\NormalTok{, }\DataTypeTok{trControl=}\KeywordTok{trainControl}\NormalTok{())}
\KeywordTok{print}\NormalTok{(modelTree)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Conditional Inference Tree 
## 
## 13737 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 13737, 13737, 13737, 13737, 13737, 13737, ... 
## Resampling results across tuning parameters:
## 
##   mincriterion  Accuracy   Kappa    
##   0.01          0.8635282  0.8273714
##   0.50          0.8633627  0.8271633
##   0.99          0.8601143  0.8230183
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mincriterion = 0.01.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{controlRf <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method=}\StringTok{"cv"}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{modelRf <-}\StringTok{ }\KeywordTok{train}\NormalTok{(classe }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data=}\NormalTok{mytrainData, }\DataTypeTok{method=}\StringTok{"rf"}\NormalTok{, }\DataTypeTok{trControl=}\NormalTok{controlRf, }\DataTypeTok{ntree=}\DecValTok{250}\NormalTok{)}
\KeywordTok{print}\NormalTok{(modelRf)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Random Forest 
## 
## 13737 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 10990, 10990, 10989, 10991, 10988 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    2    0.9919196  0.9897783
##   27    0.9919921  0.9898700
##   52    0.9851484  0.9812118
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 27.
\end{verbatim}

Analyzing the two models we can see that the randoms forest is the model
with a better accuracy.

\subsubsection{Evaluation of the model on the training
data}\label{evaluation-of-the-model-on-the-training-data}

Models is evalued on the 30\% of the training data not used for fitting.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predictTree <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(modelTree, mytestData)}
\NormalTok{accuracyTree <-}\StringTok{ }\KeywordTok{postResample}\NormalTok{(predictTree, }\KeywordTok{as.factor}\NormalTok{(mytestData}\OperatorTok{$}\NormalTok{classe))}
\NormalTok{accuracyTree}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Accuracy     Kappa 
## 0.8868309 0.8570047
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predictRf <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(modelRf, mytestData)}
\NormalTok{accuracyRf <-}\StringTok{ }\KeywordTok{postResample}\NormalTok{(predictRf,  }\KeywordTok{as.factor}\NormalTok{(mytestData}\OperatorTok{$}\NormalTok{classe))}
\NormalTok{accuracyRf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Accuracy     Kappa 
## 0.9947324 0.9933374
\end{verbatim}

The same accuracy is confirmed on the data training used for the test.

\subsubsection{Predicting for Test Data
Set}\label{predicting-for-test-data-set}

Results on the testing data, take in account that the modelRf is the
best model

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{resultRf <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(modelRf, testing)}
\KeywordTok{print}\NormalTok{(resultRf)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E
\end{verbatim}

\end{document}
